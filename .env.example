# Environment Configuration for Data Pipeline
# Copy this file to .env and modify as needed

# MinIO Configuration
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin123
MINIO_ENDPOINT=minio:9000
MINIO_CONSOLE_PORT=9001

# Airflow Configuration
AIRFLOW_UID=50000
AIRFLOW_GID=0
AIRFLOW_PROJ_DIR=./

# PostgreSQL Configuration
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow

# Spark Configuration
SPARK_MASTER_URL=spark://spark-master:7077
SPARK_WORKER_MEMORY=2G
SPARK_WORKER_CORES=2

# Data Source Configuration
SOURCE_S3_BUCKET=epf-big-data-processing
SOURCE_S3_PREFIX=tp3-4/sources

# Pipeline Configuration
PIPELINE_SCHEDULE_INTERVAL=0 */6 * * *  # Every 6 hours
DATA_RETENTION_DAYS=30

# Monitoring Configuration
HEALTH_CHECK_INTERVAL=*/15 * * * *  # Every 15 minutes

# Security Configuration (Change for production!)
FERNET_KEY=your-fernet-key-here
SECRET_KEY=your-secret-key-here

# Resource Limits
MAX_ACTIVE_RUNS=1
MAX_ACTIVE_TASKS=16
WORKER_CONCURRENCY=16

# Logging Configuration
LOG_LEVEL=INFO
ENABLE_DEBUG_LOGGING=false
